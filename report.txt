- Report Infomation Retrieval -
//Assignment 2

Victoriano Izquierdo
Student NÂº 3395032

*Ranked Retrieval

In order to be able to implement the ranked search I started by modifying some of the code I wrote in the previous for indexing. I store now in map.txt a weight for each one of the documents of the colletion indexed. For that matter I created Weights class where thanks to HashMap realWeights I can keep track of each DOC accumulated weight that I compute using the Indexer class as well (see Exect method) following the algorithm seen page 56 of the Information Retrieval lecture notes. 

Then I run the query search and basically I also followed the procedure described on page 63 of the lecture notes "Query Processing Algorithm". I process each query at a time, I call the function computeTotalAD() in the Word class that retrieve the values of the parameters needed (N, ft, fdt) using different utility functions I created to do so and ends up adding the accumulated similarity value with Accumulator.addAccu() in a HashTable called "theAccumulator" located in the Accumulator class. 

Once all the query terms has been processed and its Ad computed I call in search.java the Accumulator.computeFinalAD() to just divide Ad of a document by its corresponding peso Wd.

It's time to call Accumulator.addtoMinheap() to move the similarity values of each document to Min-heap to make more efficient the retrieving of the top ranked documents. For that matter I used the recommended data structure TreeMap<Double, String> minheap. 

Finally I call Accumulator.printTopResults(numResults, queryLabel) passing the flags parameter introduced when firing the program to identify the query with a label and limit the number of n documents to show to user as results of the search.


*Novelty Detection

Novelty detection is nothing more than an extesion of the "ranked retrieval" to rerank again the documents giving priority in the top results to those that are more original respect to the others. Therefore I implemented it after "ranked retrieval" actions were taken in the flow of the program. 

Starts in search.java by checking the boolean variable isNovelty that is initialized in the getopts() function detecting from args wether is a search with Novelty detection or not. Then I call RankedDoc.createR(ranknUsed) to create the set R that is an ArrayList of <RankedDoc> with the top r given results. RankedDoc is an object that keeps the DOC int ID, its similarity respect to the query, the maximum similarity respect to the other DOCs in R (DDmaxDifference) and the final novelty value computed. createR() not just store this -R collection of documents but also RankedDoc.initializeS() initalize the set S where manually some DOC can be kept, by default I left just the DOC with id number 2 intact. 

After that, RankedDoc.computeNovelty(lambdaNumber) is invoked. This function will traverse the R ArrayList, compare each DOC respect to the other DOC by a similarity function, keep the highest score in DDmaxDifference, in total n*n comparaisons. At the end the novelty of each DOC is computed by i.novelty = (lambda * i.similarity) - ((1- lambda) * i.DDmaxDifference) as expressed in the formula of the assignment statement. 

For the DDmaxDifference since it's wasn't very well expressed anywhere how to compute the cosine similarity of two DOCs based on their weights calculated during the indexing and stored in map.txt, the most effective way I tought was to just get the difference of both weights double sim2 = wdi-wdj . I know this might not be the best number to compare two documents but it's significative and fast. Scanning all the terms of each document and check the frequencies of its term in the other documents of the collection would be very very inneficient in term of computation time for a large collection. 

After having the novelty values I procee to put this in a min-heap (TreeMap) RankedDoc.addtoMinheap() and print the top n values in the same manner I did previously RankedDoc.printTopResults(numResults, queryLabel) 

